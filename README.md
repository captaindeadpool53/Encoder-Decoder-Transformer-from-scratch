# Encoder-Decoder-Transformer-from-scratch
- Replicated the Encoder-Decoder transformer architecture from the ['Attention is all you need'](https://arxiv.org/pdf/1706.03762) paper. 
- Used Byte-pair encoding tokenization used in GPT-2 from HuggingFace.
- Used opus_books translation dataset for translating English to French.
- Self attention and Cross attention mechanism implemented from scratch.

This is done for learning purposes.
